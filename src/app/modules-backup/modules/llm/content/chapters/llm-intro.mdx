---
title: "LLM 개요와 역사"
description: "언어 모델의 발전 과정과 LLM의 탄생 배경을 살펴봅니다."
estimatedTime: 45
objectives:
  - "언어 모델의 정의와 목적 이해"
  - "n-gram에서 신경망 언어 모델까지의 발전 과정"
  - "GPT, BERT 등 주요 LLM의 특징 파악"
  - "LLM이 가져온 패러다임 변화 이해"
---

# LLM 개요와 역사

## 서론

대규모 언어 모델(Large Language Models, LLM)은 인공지능 분야에서 가장 혁신적인 발전 중 하나입니다. 
수십억 개의 파라미터를 가진 이 거대한 신경망은 인간의 언어를 이해하고 생성하는 놀라운 능력을 보여주고 있습니다.

<Callout type="info">
  **LLM이란?**
  
  대규모 언어 모델(LLM)은 방대한 텍스트 데이터로 학습된 심층 신경망으로, 
  자연어를 이해하고 생성할 수 있는 AI 시스템입니다.
</Callout>

## 1. 언어 모델의 정의

### 1.1 언어 모델이란?

언어 모델(Language Model)은 단어의 시퀀스에 확률을 할당하는 수학적 모델입니다. 
더 간단히 말하면, 주어진 문맥에서 다음에 올 단어를 예측하는 모델입니다.

```python
# 언어 모델의 기본 개념
# P(다음 단어 | 이전 단어들)

# 예시: "오늘 날씨가 정말"
# 가능한 다음 단어들:
# - "좋다" (확률: 0.4)
# - "춥다" (확률: 0.3)
# - "덥다" (확률: 0.2)
# - "흐리다" (확률: 0.1)
```

### 1.2 언어 모델의 목적

언어 모델의 주요 목적은 다음과 같습니다:

1. **텍스트 생성**: 자연스러운 문장 생성
2. **문장 평가**: 문장의 자연스러움 점수 계산
3. **예측**: 다음 단어나 문장 예측
4. **이해**: 문맥을 파악하여 의미 추출

## 2. 언어 모델의 발전 과정

### 2.1 n-gram 모델 (1980s~)

초기 언어 모델은 통계적 방법에 기반했습니다. n-gram 모델은 연속된 n개의 단어를 하나의 단위로 보고 확률을 계산합니다.

<Alert type="info" title="n-gram 예시">
  - **Unigram (n=1)**: P(좋다)
  - **Bigram (n=2)**: P(좋다|날씨가)  
  - **Trigram (n=3)**: P(좋다|날씨가 정말)
</Alert>

**n-gram 모델의 한계:**
- 긴 문맥을 고려할 수 없음
- 보지 못한 단어 조합에 대한 처리 어려움
- 저장 공간과 계산량이 n에 따라 기하급수적으로 증가

### 2.2 신경망 언어 모델 (2003~)

Bengio et al.(2003)이 제안한 신경망 언어 모델은 단어를 연속적인 벡터 공간에 임베딩하여 표현합니다.

```python
# 신경망 언어 모델의 구조
input_words = ["나는", "학교에", "간다"]
embeddings = embedding_layer(input_words)  # 단어 → 벡터
hidden = neural_network(embeddings)         # 은닉층 계산
output = softmax(hidden)                    # 확률 분포 출력
```

**주요 장점:**
- 단어 간 의미적 유사성 학습
- 일반화 능력 향상
- 더 긴 문맥 고려 가능

### 2.3 RNN 기반 모델 (2010~)

순환 신경망(RNN)과 그 변형인 LSTM, GRU는 시퀀스 데이터를 효과적으로 처리할 수 있게 했습니다.

<Callout type="tip" emoji="💡">
  RNN은 이전 상태를 기억하여 긴 문맥을 처리할 수 있지만, 
  기울기 소실 문제로 인해 매우 긴 시퀀스는 여전히 어려웠습니다.
</Callout>

### 2.4 Transformer의 등장 (2017)

"Attention is All You Need" 논문에서 제안된 Transformer는 언어 모델의 판도를 완전히 바꿨습니다.

**Transformer의 혁신:**
- Self-Attention 메커니즘으로 병렬 처리 가능
- 긴 거리 의존성 효과적 학습
- 대규모 데이터와 모델로 확장 가능

## 3. 주요 LLM 모델들

### 3.1 GPT 시리즈 (OpenAI)

**GPT (2018)**
- 파라미터: 1.17억 개
- Unsupervised pre-training + Supervised fine-tuning
- 단방향(왼쪽→오른쪽) 언어 모델

**GPT-2 (2019)**
- 파라미터: 15억 개
- Zero-shot 태스크 수행 능력 입증
- "너무 위험해서" 초기에는 공개 보류

**GPT-3 (2020)**
- 파라미터: 1,750억 개
- Few-shot learning의 강력한 성능
- API 서비스로 상용화

**GPT-4 (2023)**
- 멀티모달 지원 (텍스트 + 이미지)
- 향상된 추론 능력
- 더 긴 컨텍스트 윈도우

### 3.2 BERT와 그 변형들 (Google)

**BERT (2018)**
- Bidirectional Encoder Representations from Transformers
- 양방향 문맥 이해
- Masked Language Modeling (MLM)

```python
# BERT의 MLM 예시
입력: "나는 [MASK]에 간다"
예측: "나는 학교에 간다"
```

**주요 변형:**
- RoBERTa: 더 많은 데이터와 긴 학습
- ALBERT: 파라미터 공유로 경량화
- ELECTRA: 효율적인 사전 학습 방법

### 3.3 기타 주요 모델들

**T5 (Google)**
- Text-to-Text 통합 프레임워크
- 모든 NLP 태스크를 텍스트 생성으로 통일

**LLaMA (Meta)**
- 효율적인 오픈소스 모델
- 다양한 크기 (7B~65B)

**Claude (Anthropic)**
- Constitutional AI로 안전성 강화
- 긴 컨텍스트 처리 능력

## 4. LLM이 가져온 패러다임 변화

### 4.1 Few-shot Learning

전통적인 머신러닝과 달리, LLM은 몇 개의 예시만으로도 새로운 태스크를 수행할 수 있습니다.

<Exercise
  id="few-shot-example"
  title="Few-shot Learning 실습"
  description="주어진 예시를 보고 패턴을 파악하여 새로운 입력에 대한 출력을 생성해보세요."
  type="interactive"
  initialCode={`# Few-shot 예시
# 긍정/부정 분류 태스크

예시1: "이 영화 정말 재미있어요!" → 긍정
예시2: "시간 낭비였습니다." → 부정
예시3: "평범한 수준이네요." → 중립

# 다음 문장을 분류해보세요:
입력: "최고의 영화였어요!"
출력: ?`}
/>

### 4.2 Emergent Abilities

모델 크기가 특정 임계점을 넘으면 예상치 못한 능력들이 나타납니다:

- **추론 능력**: 논리적 사고와 문제 해결
- **코드 생성**: 프로그래밍 언어 이해와 생성
- **다국어 이해**: 학습하지 않은 언어도 어느 정도 처리

### 4.3 프롬프트 엔지니어링

LLM과의 상호작용에서 프롬프트의 중요성이 부각되었습니다:

<Callout type="important">
  좋은 프롬프트는 LLM의 성능을 크게 향상시킬 수 있습니다.
  명확한 지시사항, 적절한 예시, 구조화된 형식이 중요합니다.
</Callout>

## 5. LLM의 현재와 미래

### 5.1 현재의 응용 분야

- **대화형 AI**: ChatGPT, Claude 등
- **코드 어시스턴트**: GitHub Copilot, Cursor
- **콘텐츠 생성**: 글쓰기, 번역, 요약
- **정보 추출**: 문서 분석, 지식 추출

### 5.2 도전 과제

1. **Hallucination**: 그럴듯하지만 잘못된 정보 생성
2. **편향성**: 학습 데이터의 편향 반영
3. **컴퓨팅 비용**: 거대한 모델의 학습과 추론 비용
4. **해석가능성**: 모델의 의사결정 과정 이해 어려움

### 5.3 미래 전망

- **효율적인 모델**: 더 작지만 강력한 모델 개발
- **멀티모달 통합**: 텍스트, 이미지, 음성, 비디오 통합 처리
- **특화된 모델**: 도메인별 전문 LLM
- **인간-AI 협업**: 더 자연스러운 상호작용

## 퀴즈

<Quiz
  id="llm-intro-quiz"
  quiz={{
    id: "llm-intro-quiz",
    questions: [
      {
        id: "q1",
        type: "multiple-choice",
        question: "Transformer 아키텍처의 주요 혁신은 무엇인가요?",
        options: [
          "RNN을 사용한 순차적 처리",
          "Self-Attention 메커니즘",
          "CNN을 사용한 특징 추출",
          "강화학습 적용"
        ],
        correctAnswer: "Self-Attention 메커니즘",
        explanation: "Transformer는 Self-Attention 메커니즘을 통해 병렬 처리와 긴 거리 의존성 학습을 가능하게 했습니다."
      },
      {
        id: "q2",
        type: "true-false",
        question: "GPT 모델은 양방향(bidirectional) 언어 모델이다.",
        correctAnswer: "거짓",
        explanation: "GPT는 단방향(left-to-right) 언어 모델입니다. BERT가 양방향 모델의 대표적인 예입니다."
      },
      {
        id: "q3",
        type: "multiple-choice",
        question: "Few-shot learning이란?",
        options: [
          "많은 데이터로 학습하는 방법",
          "적은 예시만으로 새로운 태스크를 수행하는 능력",
          "빠른 속도로 학습하는 방법",
          "여러 번 반복 학습하는 방법"
        ],
        correctAnswer: "적은 예시만으로 새로운 태스크를 수행하는 능력",
        explanation: "Few-shot learning은 LLM이 몇 개의 예시만 보고도 새로운 태스크를 수행할 수 있는 능력을 말합니다."
      }
    ]
  }}
/>

## 요약

이 장에서는 언어 모델의 기본 개념부터 시작하여 LLM의 발전 과정을 살펴보았습니다. 
n-gram에서 시작된 언어 모델은 신경망, RNN을 거쳐 Transformer에 이르러 혁명적인 발전을 이루었습니다. 
GPT, BERT 등의 주요 모델들은 각자의 특징을 가지고 다양한 분야에서 활용되고 있으며, 
Few-shot learning과 같은 새로운 패러다임을 제시했습니다.

다음 장에서는 LLM의 첫 번째 단계인 토크나이제이션에 대해 자세히 알아보겠습니다.